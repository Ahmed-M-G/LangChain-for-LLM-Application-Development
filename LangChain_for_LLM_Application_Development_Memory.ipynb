{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zkeAVm0TZfUc",
        "WNzOUV9Lc0pN",
        "f_hZktK2ee_Y",
        "Sg85KFfyfhgk"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain Memory\n",
        "In LangChain, Memory refers to a mechanism that enables the language model to retain and reference information from earlier parts of a conversation or interaction. This is particularly useful for building applications that require context persistence, such as chatbots, virtual assistants, or multi-step workflows.\n",
        "\n",
        "LangChain memory modules allow developers to:\n",
        "\n",
        "- Store and retrieve conversational history: Memory keeps track of user inputs and model responses across multiple turns.\n",
        "\n",
        "- Maintain context across interactions: This ensures that the model can refer back to previous statements, improving the coherence and relevance of responses.\n",
        "\n",
        "- Support different memory strategies: LangChain offers several types of memory, including:\n",
        "\n",
        " - `ConversationBufferMemory`: Stores the full history of the conversation as a simple buffer.\n",
        "\n",
        " - `ConversationSummaryMemory`: Summarizes past interactions to maintain context while saving memory.\n",
        "\n",
        " - `ConversationBufferWindowMemory`: Retains only the last k messages, making it suitable for short-term context windows.\n",
        "\n",
        " - `CombinedMemory`: Combines multiple memory modules for advanced use cases.\n",
        "\n",
        "By incorporating memory into a LangChain application, developers can create more intelligent, context-aware systems that provide a natural and consistent user experience."
      ],
      "metadata": {
        "id": "PMwWUWDiY9v-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting Up Work Environment"
      ],
      "metadata": {
        "id": "jOE0yGqJ6Yk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade google-generativeai"
      ],
      "metadata": {
        "collapsed": true,
        "id": "AafKYYd-wuUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google-genai"
      ],
      "metadata": {
        "id": "kvESO7_a1x1d",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community"
      ],
      "metadata": {
        "collapsed": true,
        "id": "If8Mzx0CAei-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-google-genai"
      ],
      "metadata": {
        "collapsed": true,
        "id": "L3r0Y0DNA0Up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google import genai\n",
        "import google.generativeai as ggenai\n",
        "from google.colab import userdata\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "from PIL import Image\n",
        "from google.genai import types\n",
        "\n",
        "from IPython.display import HTML"
      ],
      "metadata": {
        "id": "qdoxw_y2v5UH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "from langchain.memory import ConversationTokenBufferMemory\n",
        "\n",
        "from langchain.memory import ConversationSummaryBufferMemory"
      ],
      "metadata": {
        "id": "YuGZcIziC3rt"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the API key (Replace 'YOUR_API_KEY' with your actual Gemini API key)\n",
        "key = userdata.get('genai_api')\n",
        "client = genai.Client(api_key=key)"
      ],
      "metadata": {
        "id": "dK9gy8xMvlOV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "List the set of available models"
      ],
      "metadata": {
        "id": "wR9WQ3B76c3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ggenai.configure(api_key=key)\n",
        "\n",
        "models = ggenai.list_models()\n",
        "# for model in models:\n",
        "    # print(model.name)"
      ],
      "metadata": {
        "id": "DKPmEKApyY0Q",
        "collapsed": true
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Gemini LLM\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",       # or \"gemini-1.5-pro\", etc.\n",
        "    temperature=0.0,\n",
        "    google_api_key=key\n",
        ")"
      ],
      "metadata": {
        "id": "BAucEjNAaJpo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversation Buffer Memory\n",
        "\n",
        "is one of the simplest and most commonly used memory modules in LangChain. It maintains a complete, sequential history of the conversation between the user and the language model. This buffer captures all prior inputs and responses, enabling the model to refer back to earlier parts of the dialogue when generating new responses."
      ],
      "metadata": {
        "id": "zkeAVm0TZfUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Conversation memory\n",
        "memory = ConversationBufferMemory()\n",
        "# Conversation chain\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzndq378Zg4X",
        "outputId": "68d4b80d-6fc9-4fce-8085-073ca6a78585"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-10-59686291.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory()\n",
            "/tmp/ipython-input-10-59686291.py:5: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
            "  conversation = ConversationChain(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Hi, my name is Gaber\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "byMK9Tl4aasf",
        "outputId": "761dde50-e218-4d3e-9da7-6e203e9acd96"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi, my name is Gaber\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hi Gaber! It\\'s nice to meet you.  My name isn\\'t really a \"name\" in the human sense, as I don\\'t have a personal identity like you do.  I\\'m a large language model, trained by Google.  I don\\'t have feelings or experiences, but I can access and process information from a massive dataset of text and code.  So, tell me, what can I do for you today?  Are you interested in learning something new, writing a story, translating something, or perhaps just having a chat?  I\\'m ready for anything!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What is 1+1?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "s8rxlFXGatRp",
        "outputId": "0d619879-4f20-4453-fdd0-3473ace331dc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Gaber\n",
            "AI: Hi Gaber! It's nice to meet you.  My name isn't really a \"name\" in the human sense, as I don't have a personal identity like you do.  I'm a large language model, trained by Google.  I don't have feelings or experiences, but I can access and process information from a massive dataset of text and code.  So, tell me, what can I do for you today?  Are you interested in learning something new, writing a story, translating something, or perhaps just having a chat?  I'm ready for anything!\n",
            "Human: What is 1+1?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"1 + 1 = 2.  That's a pretty straightforward calculation!  It's a fundamental concept in arithmetic, forming the basis for more complex mathematical operations.  Interestingly, the concept of addition, while seemingly simple, has a rich history, evolving from early counting methods to the sophisticated mathematical systems we use today.  Are you interested in learning more about the history of mathematics, or perhaps exploring some more complex mathematical problems?  I'm happy to delve into whatever you'd like!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What is my name?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "Pv6EfCcabWJu",
        "outputId": "d42acad4-18ac-453f-8140-d7c3882ee970"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Gaber\n",
            "AI: Hi Gaber! It's nice to meet you.  My name isn't really a \"name\" in the human sense, as I don't have a personal identity like you do.  I'm a large language model, trained by Google.  I don't have feelings or experiences, but I can access and process information from a massive dataset of text and code.  So, tell me, what can I do for you today?  Are you interested in learning something new, writing a story, translating something, or perhaps just having a chat?  I'm ready for anything!\n",
            "Human: What is 1+1?\n",
            "AI: 1 + 1 = 2.  That's a pretty straightforward calculation!  It's a fundamental concept in arithmetic, forming the basis for more complex mathematical operations.  Interestingly, the concept of addition, while seemingly simple, has a rich history, evolving from early counting methods to the sophisticated mathematical systems we use today.  Are you interested in learning more about the history of mathematics, or perhaps exploring some more complex mathematical problems?  I'm happy to delve into whatever you'd like!\n",
            "Human: What is my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Your name is Gaber.  We established that at the beginning of our conversation.  I have a pretty good memory (within the constraints of my current session, of course!), so I'm able to recall details from our interaction. Is there anything else I can help you with? Perhaps you'd like to explore a different topic, or maybe try a more challenging question?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This prompt was generated by LangChain to guide the system toward maintaining a hopeful and friendly tone during the conversation. When the conversation continues through subsequent inputs (such as the second and third turns), the prompt remains consistent.\n",
        "\n",
        "Since a memory object was used to retain conversational history, printing `memory.buffer` will display the stored dialogue up to that point. This allows the model to retain context and respond appropriately based on the ongoing interaction."
      ],
      "metadata": {
        "id": "0Xel7B8LbpCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwdhzWZgbZ8Q",
        "outputId": "de6c027e-d258-4645-f96c-a12ad0128e35"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Hi, my name is Gaber\n",
            "AI: Hi Gaber! It's nice to meet you.  My name isn't really a \"name\" in the human sense, as I don't have a personal identity like you do.  I'm a large language model, trained by Google.  I don't have feelings or experiences, but I can access and process information from a massive dataset of text and code.  So, tell me, what can I do for you today?  Are you interested in learning something new, writing a story, translating something, or perhaps just having a chat?  I'm ready for anything!\n",
            "Human: What is 1+1?\n",
            "AI: 1 + 1 = 2.  That's a pretty straightforward calculation!  It's a fundamental concept in arithmetic, forming the basis for more complex mathematical operations.  Interestingly, the concept of addition, while seemingly simple, has a rich history, evolving from early counting methods to the sophisticated mathematical systems we use today.  Are you interested in learning more about the history of mathematics, or perhaps exploring some more complex mathematical problems?  I'm happy to delve into whatever you'd like!\n",
            "Human: What is my name?\n",
            "AI: Your name is Gaber.  We established that at the beginning of our conversation.  I have a pretty good memory (within the constraints of my current session, of course!), so I'm able to recall details from our interaction. Is there anything else I can help you with? Perhaps you'd like to explore a different topic, or maybe try a more challenging question?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also display the stored conversation by using memory.`load_memory_variables({})`. The empty dictionary {} is used as a default input in this case. While LangChain supports more advanced usage with customized inputs, those features are beyond the scope of this article and can be explored further in the official LangChain documentation.\n",
        "\n",
        "For now, you can disregard the reason behind passing an empty dictionary. The key takeaway is that this method retrieves everything the AI and the human have said so far, representing the complete conversation history stored in memory."
      ],
      "metadata": {
        "id": "G3Zqm6rrb5qd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVwtp0D9bxEk",
        "outputId": "963c48b9-c37e-4150-d20c-f331bc730f06"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'Human: Hi, my name is Gaber\\nAI: Hi Gaber! It\\'s nice to meet you.  My name isn\\'t really a \"name\" in the human sense, as I don\\'t have a personal identity like you do.  I\\'m a large language model, trained by Google.  I don\\'t have feelings or experiences, but I can access and process information from a massive dataset of text and code.  So, tell me, what can I do for you today?  Are you interested in learning something new, writing a story, translating something, or perhaps just having a chat?  I\\'m ready for anything!\\nHuman: What is 1+1?\\nAI: 1 + 1 = 2.  That\\'s a pretty straightforward calculation!  It\\'s a fundamental concept in arithmetic, forming the basis for more complex mathematical operations.  Interestingly, the concept of addition, while seemingly simple, has a rich history, evolving from early counting methods to the sophisticated mathematical systems we use today.  Are you interested in learning more about the history of mathematics, or perhaps exploring some more complex mathematical problems?  I\\'m happy to delve into whatever you\\'d like!\\nHuman: What is my name?\\nAI: Your name is Gaber.  We established that at the beginning of our conversation.  I have a pretty good memory (within the constraints of my current session, of course!), so I\\'m able to recall details from our interaction. Is there anything else I can help you with? Perhaps you\\'d like to explore a different topic, or maybe try a more challenging question?'}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- LangChain utilizes ConversationBufferMemory to store the conversation history. If you wish to explicitly add new inputs and outputs to the memory, you can do so by directly interacting with the memory object. This allows you to manually record specific interactions.\n",
        "\n",
        "- This is the standard approach to programmatically append new exchanges to the memory, ensuring they are available for future context-aware interactions."
      ],
      "metadata": {
        "id": "GlG4YnlWcM-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferMemory()\n",
        "memory.save_context({\"input\": \"Hi\"},\n",
        "                    {\"output\": \"What's up\"})"
      ],
      "metadata": {
        "id": "UzIzInWTcBLb"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opIhpDIfcY5W",
        "outputId": "117f3874-d9bb-4787-b04d-ea09db39b9e7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Hi\n",
            "AI: What's up\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUN_YfCpcaro",
        "outputId": "e5e2daa6-5715-4b70-f969-a7b650dc656e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi\\nAI: What's up\"}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversation Buffer Window Memory\n",
        "\n",
        "ConversationBufferWindowMemory is a variant of LangChain’s memory module designed to manage conversational history more efficiently by retaining only a fixed number of the most recent exchanges. Unlike `ConversationBufferMemory`, which stores the entire dialogue, this memory type helps avoid exceeding token limits and keeps the context focused on the most relevant parts of the conversation."
      ],
      "metadata": {
        "id": "WNzOUV9Lc0pN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferWindowMemory(k=1)\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=False\n",
        ")"
      ],
      "metadata": {
        "id": "3Cw6ezZ-cuht"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.save_context({\"input\": \"Hi\"},\n",
        "                    {\"output\": \"What's up\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})"
      ],
      "metadata": {
        "id": "PEFt3sz9dMV0"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Cea8rXRdMxv",
        "outputId": "b5d45b5a-447c-46de-d2c8-98b30a09b9d2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the value of k was set to 1, only the most recent exchange is retained in memory. This behavior is useful when you want the system to focus solely on the latest part of the conversation. However, in practice, k is typically set to a larger number to retain a broader context.\n",
        "\n",
        "For example, if we rerun the earlier conversation with k = 1, we might start with the input: “Hi, my name is Gaber,” followed by: “What is 1 plus 1?”, and finally ask: “What is my name?”. Because only the last exchange is remembered, the model recalls the most recent question and response—“What is 1 plus 1?” and “1 plus 1 equals 2”—but forgets the initial message where the name was provided. As a result, when asked to recall the name, the model responds that it does not have access to that information.\n",
        "\n",
        "This illustrates how ConversationBufferWindowMemory limits the context window to improve efficiency while sacrificing long-term memory."
      ],
      "metadata": {
        "id": "wvIeaqSUd5-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Hi, my name is Gaber\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "pGxNvEUadpYd",
        "outputId": "a88c0a6b-e1d8-44ad-a106-cf4f316aa0f7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hi Gaber! It's nice to meet you.  Is there anything I can help you with today?  I'm ready for a chat about pretty much anything, from the history of the Roman Empire to the best way to bake a chocolate chip cookie (though I can't actually *bake* anything, of course!).  Let me know what's on your mind.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What is 1+1?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "KWpjGhuOeUOM",
        "outputId": "80a41719-718f-4e0e-d052-c281cb482778"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"1 + 1 = 2.  That's a pretty straightforward one!  It's a fundamental concept in arithmetic, the branch of mathematics dealing with numbers and their operations.  This particular equation is an example of addition, where we're combining two quantities.  In this case, we're combining one unit with another unit to get a total of two units.  It's the basis for much more complex mathematical calculations, and you'll find it used everywhere from simple counting to advanced calculus.  Anything else I can help you with today, Gaber?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What is my name?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "eMxUwYSYeXzI",
        "outputId": "6a628ce5-bd42-4c0d-f4cb-9756e74ee5e8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I don\\'t know your name.  You haven\\'t told me.  My previous response called you \"Gaber,\" but that was just a guess based on common names and the fact that I\\'m trying to be conversational.  I don\\'t have access to personal information about you unless you explicitly provide it.  What\\'s your name?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversation Token Buffer Memory\n",
        "\n",
        "ConversationTokenBufferMemory is a specialized memory type in LangChain that retains recent interactions based on token count, rather than a fixed number of message turns. This is particularly useful when working with large language models that have strict token limits for inputs."
      ],
      "metadata": {
        "id": "f_hZktK2ee_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n",
        "\n",
        "memory.save_context({\"input\": \"AI is what?!\"},\n",
        "                    {\"output\": \"Amazing!\"})\n",
        "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
        "                    {\"output\": \"Beautiful!\"})\n",
        "memory.save_context({\"input\": \"Chatbots are what?\"},\n",
        "                    {\"output\": \"Charming!\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HtxV6RdeaGu",
        "outputId": "8b4726fe-aa73-4928-b2e0-60082c7fc663"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-35-2362117468.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gA9p_hheuIy",
        "outputId": "b4324f43-8b6b-4292-d8cf-70d307b75b16"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'Human: AI is what?!\\nAI: Amazing!\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- When executed with a high token limit, the memory retains nearly the entire conversation. By increasing the token limit to 50, it is now able to store the full conversation history."
      ],
      "metadata": {
        "id": "YMPIT_n-fDwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=20)\n",
        "memory.save_context({\"input\": \"AI is what?!\"},\n",
        "                    {\"output\": \"Amazing!\"})\n",
        "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
        "                    {\"output\": \"Beautiful!\"})\n",
        "memory.save_context({\"input\": \"Chatbots are what?\"},\n",
        "                    {\"output\": \"Charming!\"})\n",
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmW-Taa4ewo3",
        "outputId": "75941651-73ee-4db2-8c4f-7be38854c6d2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'AI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- If the token limit is reduced to 20, the memory truncates the earlier parts of the conversation, preserving only the most recent exchanges. This ensures that the total number of tokens remains within the specified limit, allowing the system to maintain context without exceeding the allowed token budget."
      ],
      "metadata": {
        "id": "ccHqOHrZfW6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversation Summary Memory\n",
        "\n",
        "ConversationSummaryMemory is a memory module in LangChain that retains the conversation history in the form of a running summary, rather than storing the entire dialogue verbatim. This approach is especially useful when working within strict token limits or when maintaining a high-level understanding of the conversation is more important than recalling exact phrasing."
      ],
      "metadata": {
        "id": "Sg85KFfyfhgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schedule = \"There is a meeting at 8am with your product team. \\\n",
        "You will need your powerpoint presentation prepared. \\\n",
        "9am-12pm have time to work on your LangChain \\\n",
        "project which will go quickly because Langchain is such a powerful tool. \\\n",
        "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
        "from over an hour away to meet you to understand the latest in AI. \\\n",
        "Be sure to bring your laptop to show the latest LLM demo.\""
      ],
      "metadata": {
        "id": "ZPlDVLbIfQQt"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In this example, we use ConversationSummaryBufferMemory with a maximum token limit of 400, which allows for a relatively large amount of conversational context to be retained. We simulate a dialogue that begins with several short exchanges—such as “Hello,” “What’s up?” “Not much, just hanging,” “Cool,” and “What is on the schedule today?”—followed by a longer response detailing the schedule.\n",
        "\n",
        "- Given the generous token limit, the memory is able to store a substantial amount of content. When we inspect the memory variables, we can see that the entire conversation, including the lengthy response, has been preserved—demonstrating that 400 tokens were sufficient to capture the full interaction."
      ],
      "metadata": {
        "id": "j6-bGiz7f9El"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=400)\n",
        "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})\n",
        "memory.save_context({\"input\": \"What is on the schedule today?\"},\n",
        "                    {\"output\": f\"{schedule}\"})"
      ],
      "metadata": {
        "id": "O5d6VQRLgmgS"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TUAn5Legpv-",
        "outputId": "c921ccfc-548a-41f0-dc5d-57e9ffc4c663"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hello\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\\nHuman: What is on the schedule today?\\nAI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- If the maximum token limit is reduced to 100, the ConversationSummaryBufferMemory will invoke a language model—such as the Gemini endpoint in this case—to generate a concise summary of the conversation. This summary replaces the full text of previous exchanges, allowing the memory to retain essential context while remaining within the specified token limit. The resulting summary is shown below."
      ],
      "metadata": {
        "id": "dQmTlu5LgkyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
        "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})\n",
        "memory.save_context({\"input\": \"What is on the schedule today?\"},\n",
        "                    {\"output\": f\"{schedule}\"})"
      ],
      "metadata": {
        "id": "Pl_aqOjEfoHE"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OItc5vjfgMG_",
        "outputId": "5cbbd960-a4ce-427b-aa6d-c7e9dc55067e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'System: The human greets the AI, and after some casual exchange, asks what is on the schedule for the day.\\nAI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.'}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "OM7aJUI1gONs"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What would be a good demo to show?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "S-ISXiXog40_",
        "outputId": "5034ff06-d8b4-481b-de6e-3a6ee4cdbaa6"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "System: The human greets the AI, and after some casual exchange, asks what is on the schedule for the day.\n",
            "AI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\n",
            "Human: What would be a good demo to show?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"That's a great question!  Since your customer is driving over an hour to learn about the latest in AI, I'd recommend focusing on a demo that showcases the practical applications of LLMs and is relatively easy to understand, even for someone without a deep technical background.  Here are a few options, ranked in order of my recommendation:\\n\\n1. **A customized question-answering system using LangChain:** This directly relates to your LangChain project and highlights its power. You could pre-load it with information relevant to your customer's industry or needs.  The demo could involve asking it a few insightful questions, demonstrating its ability to synthesize information and provide concise, accurate answers.  This shows the practical application of LLMs in a business context.  Make sure to choose questions that highlight the benefits for *them* specifically.\\n\\n2. **A simple text summarization demo:**  This is visually appealing and easy to grasp.  You could feed it a longer document (perhaps a news article relevant to their industry) and show how quickly and accurately it summarizes the key points. This demonstrates the time-saving potential of LLMs.\\n\\n3. **A creative text generation demo (with caveats):**  You could show it generating different creative text formats, like poems, scripts, or marketing copy. However, be cautious here.  The output might not always be perfect, and you need to manage expectations.  Frame this as showcasing the *potential* of LLMs for creative tasks, rather than a fully polished product.  This is less practical than the other options.\\n\\n**Important Considerations for ALL demos:**\\n\\n* **Keep it concise:**  Aim for a 5-10 minute demo, max.  Respect your customer's time.\\n* **Focus on the value proposition:**  Clearly show how the LLM technology can benefit *your customer*.  Connect the demo to their specific needs and pain points.\\n* **Have a backup plan:**  Network issues or unexpected LLM behavior can happen.  Have a simple alternative ready, or be prepared to explain any hiccups transparently.\\n* **Prepare talking points:**  Don't just show the demo; explain what's happening, why it's impressive, and how it relates to your customer's business.\\n\\n\\nI don't know the specifics of your customer's business, so I can't give a perfectly tailored recommendation.  However, I believe option 1 (the customized question-answering system) is the strongest choice given the context.  It directly showcases your LangChain work and its practical application in a business setting.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "| Feature                     | ConversationBufferMemory | ConversationBufferWindowMemory | ConversationSummaryMemory |\n",
        "| --------------------------- | ------------------------ | ------------------------------ | ------------------------- |\n",
        "| Retention method            | Full history             | Last *k* message pairs         | Summarized context        |\n",
        "| Token efficiency            | Low                      | Medium                         | High                      |\n",
        "| Retains exact phrasing      | Yes                      | Yes (for last *k*)             | No                        |\n",
        "| Suitable for long dialogues | No                       | Limited                        | Yes                       |\n"
      ],
      "metadata": {
        "id": "_2mbbwfmhC1r"
      }
    }
  ]
}