{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jOE0yGqJ6Yk9",
        "7-jIfvTQoXc2",
        "7lQNV6ArqIEX",
        "QMyleualsMIe",
        "6BcI-Gbjt1mj"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain Chains\n",
        "\n",
        "In LangChain, chains are modular sequences of components that automate and structure interactions with language models (LLMs). A chain defines the flow of data—from input processing and prompt construction to LLM execution and output parsing—allowing developers to build sophisticated LLM-driven applications with clarity and reusability.\n",
        "\n",
        "Rather than calling an LLM directly with a prompt, chains help encapsulate logic, dependencies, and additional features like memory, tools, or post-processing.\n",
        "\n",
        "Types of Chains in LangChain\n",
        "\n",
        "- Simple Chains (LLMChain)\n",
        "- Sequential Chains\n",
        "- Router Chains\n",
        "- MultiPromptChain\n",
        "- Custom Chains"
      ],
      "metadata": {
        "id": "FT4N1PPInyyq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting Up Work Environment"
      ],
      "metadata": {
        "id": "jOE0yGqJ6Yk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade google-generativeai"
      ],
      "metadata": {
        "collapsed": true,
        "id": "AafKYYd-wuUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google-genai"
      ],
      "metadata": {
        "id": "kvESO7_a1x1d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community"
      ],
      "metadata": {
        "collapsed": true,
        "id": "If8Mzx0CAei-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-google-genai"
      ],
      "metadata": {
        "collapsed": true,
        "id": "L3r0Y0DNA0Up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google import genai\n",
        "import google.generativeai as ggenai\n",
        "from google.colab import userdata\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "import pandas as pd\n",
        "\n",
        "from PIL import Image\n",
        "from google.genai import types\n",
        "\n",
        "from IPython.display import HTML"
      ],
      "metadata": {
        "id": "qdoxw_y2v5UH"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "from langchain.chains import SequentialChain\n",
        "\n",
        "from langchain.chains.router import MultiPromptChain\n",
        "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "YuGZcIziC3rt"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the API key (Replace 'YOUR_API_KEY' with your actual Gemini API key)\n",
        "key = userdata.get('genai_api')\n",
        "client = genai.Client(api_key=key)"
      ],
      "metadata": {
        "id": "dK9gy8xMvlOV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "List the set of available models"
      ],
      "metadata": {
        "id": "wR9WQ3B76c3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ggenai.configure(api_key=key)\n",
        "\n",
        "models = ggenai.list_models()\n",
        "# for model in models:\n",
        "#     print(model.name)"
      ],
      "metadata": {
        "id": "DKPmEKApyY0Q",
        "collapsed": true
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Gemini LLM\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",       # or \"gemini-1.5-pro\", etc.\n",
        "    temperature=0.9,\n",
        "    google_api_key=key\n",
        ")"
      ],
      "metadata": {
        "id": "ntFXfFTuonPW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Chains (LLMChain)\n",
        "\n",
        "The LLMChain is the simplest and most fundamental type of chain in LangChain. It is designed to streamline the process of sending inputs through a prompt template to a large language model (LLM) and returning the result. This structure is ideal for use cases where a single prompt is sufficient to generate a response."
      ],
      "metadata": {
        "id": "7-jIfvTQoXc2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We will now initialize a prompt that accepts a variable named product. This prompt will instruct the language model to generate the most suitable name for a company that produces the specified product."
      ],
      "metadata": {
        "id": "JBRjQ_8zpFh7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "bK4ltqgnnj-O"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"What is the best name to describe \\\n",
        "    a company that makes {product}?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = LLMChain(llm=llm, prompt=prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpczmrJfpMsM",
        "outputId": "da0d6d72-add5-43b6-b535-6e4a65553942"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-14-1305865249.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(llm=llm, prompt=prompt)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "product = \"Queen Size Sheet Set\""
      ],
      "metadata": {
        "id": "QbCRPNKVpNGs"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"product\": product})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLD764L5p7yK",
        "outputId": "69fb975c-76b2-41a1-dfd6-1596bb591aef"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'product': 'Queen Size Sheet Set',\n",
              " 'text': 'The best name will depend on your brand\\'s desired image and target audience.  Here are some options categorized by approach:\\n\\n**Elegant & Upscale:**\\n\\n* Royal Rest\\n* Empress Linens\\n* Sovereign Sheets\\n* Regal Sleep\\n* Velvet Slumber\\n* The Queen\\'s Collection\\n\\n**Modern & Minimalist:**\\n\\n* Queen Sheets\\n* Slumber Co.\\n* Bed & Thread\\n* Linen & Loom\\n* The Sheet Society\\n* Restful Nights\\n\\n**Cozy & Comfortable:**\\n\\n* Cozy Queen\\n* Sweet Dreams Sheets\\n* Cloud Nine Linens\\n* Slumberland Sheets\\n* The Comfort Collection\\n* Peaceful Nights\\n\\n\\n**Creative & Playful:**\\n\\n* Queen\\'s Quarters\\n* Sheet Happens\\n* The Big Sleep\\n* Dream Weaver Linens\\n\\n\\n**Tips for Choosing:**\\n\\n* **Check for availability:** Make sure the name isn\\'t already taken (website domain, trademark).\\n* **Keep it short and memorable:**  Easy to recall and share.\\n* **Reflect your brand:** Does it align with your values and target audience?\\n* **Say it out loud:** Does it sound good? Is it easy to pronounce?\\n\\n\\nConsider also incorporating keywords like \"linen,\" \"cotton,\" \"organic,\" or \"luxury\" if those aspects are important to your brand.'}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"product\": product})['text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "94x2Mmpepjph",
        "outputId": "75b234dd-8001-458b-8a43-b898130a1377"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The best name depends on the desired brand image. Here are some options, categorized by the image they project:\\n\\n**Luxury & High-End:**\\n\\n* Royal Rest\\n* Empress Linens\\n* Sovereign Sheets\\n* Regal Slumber\\n* The Queen's Chamber\\n\\n**Modern & Minimalist:**\\n\\n* Queen Set\\n* Slumber & Co.\\n* Linen & Thread\\n* The Sheet Co.\\n* Restful Nights\\n\\n**Cozy & Comfortable:**\\n\\n* Cozy Queen\\n* Sweet Dreams Sheets\\n* Dream Weaver Linens\\n* The Slumber Nook\\n* Cloud Nine Bedding\\n\\n**Playful & Unique:**\\n\\n* Queen Bee Sheets\\n* The Queen's Bed\\n* Sheet Happens\\n* Zzz's & Co.\\n* Slumber Party Linens\\n\\n\\n**Things to consider when choosing:**\\n\\n* **Target audience:** Who are you trying to reach?  A younger audience might appreciate a playful name, while an older audience might prefer something more sophisticated.\\n* **Brand personality:**  Do you want to be seen as luxurious, affordable, playful, or sophisticated?\\n* **Availability:** Check if the name is available as a website domain and trademark.\\n\\n\\nI recommend brainstorming further using these categories as a starting point and then checking for name availability.  Good luck!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sequential Chains\n",
        "\n",
        "SequentialChain is an advanced chain type in LangChain that allows multiple chains to be executed in a defined sequence, while managing multiple input and output variables across steps. Unlike SimpleSequentialChain, which only handles a single string input and output, SequentialChain supports complex workflows where each step can depend on multiple named variables and produce multiple outputs."
      ],
      "metadata": {
        "id": "Fxb9juN7qVFN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Sequential Chains\n",
        "\n",
        "SimpleSequentialChain is a type of chain in LangChain that allows you to execute multiple LLM chains in a fixed, linear order. The output of one chain is automatically passed as the input to the next. This structure is ideal for straightforward, step-by-step workflows where each step builds directly on the result of the previous one."
      ],
      "metadata": {
        "id": "7lQNV6ArqIEX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We will begin by creating the first chain, which consists of a language model and a prompt. This prompt will accept a product as input and generate an appropriate name for a company that produces that product.\n",
        "\n",
        "- Next, we will define a second chain. This chain will take the generated company name as input and produce a concise, 20-word description of the company."
      ],
      "metadata": {
        "id": "8PYu5o7xqtXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt template 1\n",
        "first_prompt = ChatPromptTemplate.from_template(\n",
        "    \"What is the best name to describe \\\n",
        "    a company that makes {product}?\"\n",
        ")\n",
        "\n",
        "# Chain 1\n",
        "chain_one = LLMChain(llm=llm, prompt=first_prompt)"
      ],
      "metadata": {
        "id": "oBaDM9CIp5zn"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt template 2\n",
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Write a 20 words description for the following \\\n",
        "    company:{company_name}\"\n",
        ")\n",
        "# chain 2\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
      ],
      "metadata": {
        "id": "uxptaHilri5Q"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two], verbose=True)"
      ],
      "metadata": {
        "id": "Cm_re14qq_fW"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_simple_chain.run(product)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sozts2DZrn5_",
        "outputId": "3606b30b-aea1-4ffa-e5ff-9f081a2a3e3c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mThe best name will depend on your brand's desired image and target audience. Here are some options, categorized by approach:\n",
            "\n",
            "**Luxury & High-End:**\n",
            "\n",
            "* Royal Rest\n",
            "* Empress Linens\n",
            "* Sovereign Sheets\n",
            "* The Queen's Chamber\n",
            "* Serene Slumber (implies luxury and comfort)\n",
            "* Celestial Sleep\n",
            "\n",
            "**Simple & Direct:**\n",
            "\n",
            "* Queen Size Sheets\n",
            "* Queen Sheets Direct\n",
            "* The Queen's Set\n",
            "* Simply Queen\n",
            "* Cozy Queen\n",
            "\n",
            "**Modern & Chic:**\n",
            "\n",
            "* Slumber & Style\n",
            "* Thread & Bloom\n",
            "* Linen & Luxe\n",
            "* The Sheet Society\n",
            "* Dream Weaver Linens\n",
            "\n",
            "**Playful & Approachable:**\n",
            "\n",
            "* Queen Bee Sheets\n",
            "* Sleepy Queen\n",
            "* Night Owl Linens\n",
            "* The Comfy Queen\n",
            "* Sweet Dreams Sheets\n",
            "\n",
            "**Focusing on a specific material:**\n",
            "\n",
            "* [Material] Queen Sheets (e.g., Egyptian Cotton Queen Sheets, Bamboo Queen Sheets)\n",
            "\n",
            "\n",
            "**Tips for choosing the best name:**\n",
            "\n",
            "* **Check for availability:** Make sure the name isn't already in use and that the domain name is available.\n",
            "* **Keep it short and memorable:** Easy to recall and pronounce.\n",
            "* **Reflect your brand values:**  Luxury, affordability, eco-friendliness, etc.\n",
            "* **Target your audience:** Consider who you're trying to reach.\n",
            "* **Get feedback:** Ask others for their opinions on your favorite names.\n",
            "\n",
            "\n",
            "Ultimately, the \"best\" name is subjective. Consider testing a few names with your target audience to see which resonates the most.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3mLuxury bedding for discerning sleepers.  Elegant designs, superior comfort, exceptional quality.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Luxury bedding for discerning sleepers.  Elegant designs, superior comfort, exceptional quality.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complex Sequential Chain (SequentialChain)\n",
        "\n",
        "The SequentialChain in LangChain—often referred to as a complex sequential chain—is a powerful and flexible tool for executing multiple language model (LLM) steps in a defined sequence. Unlike SimpleSequentialChain, which only handles a single input and output, SequentialChain supports multiple named input and output variables, allowing for rich, multi-step workflows."
      ],
      "metadata": {
        "id": "QMyleualsMIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('Data.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Za-j6ueNtgvi",
        "outputId": "c512dbbe-a11c-430b-d7dd-9e330aa7dcbe"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   Product                                             Review\n",
              "0     Queen Size Sheet Set  I ordered a king size set. My only criticism w...\n",
              "1   Waterproof Phone Pouch  I loved the waterproof sac, although the openi...\n",
              "2      Luxury Air Mattress  This mattress had a small hole in the top of i...\n",
              "3           Pillows Insert  This is the best throw pillow fillers on Amazo...\n",
              "4  Milk Frother Handheld\\n   I loved this product. But they only seem to l..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-49f28eee-1e5f-49d3-a3ea-5a2f0abfc28c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Product</th>\n",
              "      <th>Review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Queen Size Sheet Set</td>\n",
              "      <td>I ordered a king size set. My only criticism w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Waterproof Phone Pouch</td>\n",
              "      <td>I loved the waterproof sac, although the openi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Luxury Air Mattress</td>\n",
              "      <td>This mattress had a small hole in the top of i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Pillows Insert</td>\n",
              "      <td>This is the best throw pillow fillers on Amazo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Milk Frother Handheld\\n</td>\n",
              "      <td>I loved this product. But they only seem to l...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-49f28eee-1e5f-49d3-a3ea-5a2f0abfc28c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-49f28eee-1e5f-49d3-a3ea-5a2f0abfc28c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-49f28eee-1e5f-49d3-a3ea-5a2f0abfc28c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-823cde96-2571-451c-a2ad-360423e11c98\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-823cde96-2571-451c-a2ad-360423e11c98')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-823cde96-2571-451c-a2ad-360423e11c98 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 7,\n  \"fields\": [\n    {\n      \"column\": \"Product\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"Queen Size Sheet Set\",\n          \"Waterproof Phone Pouch\",\n          \"L'Or Espresso Caf\\u00e9\\u00a0\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"I ordered a king size set. My only criticism would be that I wish seller would offer the king size set with 4 pillowcases. I separately ordered a two pack of pillowcases so I could have a total of four. When I saw the two packages, it looked like the color did not exactly match. Customer service was excellent about sending me two more pillowcases so I would have four that matched. Excellent! For the cost of these sheets, I am satisfied with the characteristics and coolness of the sheets.\",\n          \"I loved the waterproof sac, although the opening was made of a hard plastic. I don\\u2019t know if that would break easily. But I couldn\\u2019t turn my phone on, once it was in the pouch.\",\n          \"Je trouve le go\\u00fbt m\\u00e9diocre. La mousse ne tient pas, c'est bizarre. J'ach\\u00e8te les m\\u00eames dans le commerce et le go\\u00fbt est bien meilleur...\\nVieux lot ou contrefa\\u00e7on !?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be using the previously defined data, which contains a customer review.\n",
        "\n",
        "- `First Chain`: This step will translate the original review into English.\n",
        "\n",
        "- `Second Chain`: Using the translated English text from the first chain, this step will generate a one-sentence summary of the review.\n",
        "\n",
        "- `Third Chain`: This step will identify the original language of the review. It operates directly on the initial review input.\n",
        "\n",
        "- `Fourth Chain`: The final step takes multiple inputs—the summary produced by the second chain and the language identified in the third chain—and generates an appropriate follow-up response written in the detected language.\n",
        "\n",
        "This sequential flow ensures that each chain builds on the outputs of previous steps, enabling a cohesive, multi-stage processing pipeline."
      ],
      "metadata": {
        "id": "Xf9xkkZds575"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt template 1: translate to english\n",
        "first_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Translate the following review to english:\"\n",
        "    \"\\n\\n{Review}\"\n",
        ")\n",
        "# chain 1: input= Review and output= English_Review\n",
        "chain_one = LLMChain(llm=llm, prompt=first_prompt, output_key=\"English_Review\")"
      ],
      "metadata": {
        "id": "JyT2pzkGrQek"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Can you summarize the following review in 1 sentence:\"\n",
        "    \"\\n\\n{English_Review}\"\n",
        ")\n",
        "# chain 2: input= English_Review and output= summary\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt, output_key=\"summary\")"
      ],
      "metadata": {
        "id": "MOVFttfJscrx"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt template 3: translate to english\n",
        "third_prompt = ChatPromptTemplate.from_template(\n",
        "    \"What language is the following review:\\n\\n{Review}\"\n",
        ")\n",
        "# chain 3: input= Review and output= language\n",
        "chain_three = LLMChain(llm=llm, prompt=third_prompt, output_key=\"language\")"
      ],
      "metadata": {
        "id": "F6jWupaDskQI"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt template 4: follow up message\n",
        "fourth_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Write a follow up response to the following \"\n",
        "    \"summary in the specified language:\"\n",
        "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
        ")\n",
        "# chain 4: input= summary, language and output= followup_message\n",
        "chain_four = LLMChain(llm=llm, prompt=fourth_prompt, output_key=\"followup_message\")"
      ],
      "metadata": {
        "id": "dpn3lld2st0V"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# overall_chain: input= Review\n",
        "# and output= English_Review,summary, followup_message\n",
        "overall_chain = SequentialChain(\n",
        "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
        "    input_variables=[\"Review\"],\n",
        "    output_variables=[\"English_Review\", \"summary\",\"followup_message\"],\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "2eCKRtMXtGJa"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review = df.Review[5]\n",
        "overall_chain(review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwDBo3gftVqJ",
        "outputId": "c571101b-373f-4e6a-af8e-d6ec656bcaca"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-44-1992003631.py:2: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  overall_chain(review)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Review': \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\",\n",
              " 'English_Review': \"I find the taste mediocre. The head doesn't hold, it's strange. I buy the same ones in stores and the taste is much better...\\n\\nOld batch or counterfeit!?\",\n",
              " 'summary': \"The reviewer found the beer's taste inferior to others they've had, suspecting it may be old or counterfeit.\",\n",
              " 'followup_message': \"Plusieurs options sont possibles, dépendant du ton souhaité :\\n\\n**Option 1 (Formel et poli) :**\\n\\n> Nous vous remercions pour votre commentaire.  Nous regrettons que la dégustation de notre bière ne vous ait pas pleinement satisfait.  Nous prenons vos soupçons concernant la fraîcheur ou l'authenticité du produit très au sérieux et allons mener une enquête interne.  Pourriez-vous nous fournir plus de détails, tels que le numéro de lot et le lieu d'achat ?\\n\\n**Option 2 (Plus informel et engageant) :**\\n\\n>  Oh zut ! On est vraiment désolés d'apprendre que vous n'avez pas apprécié notre bière.  Votre remarque concernant la possible altération ou contrefaçon nous inquiète.  Pour mieux comprendre la situation, pourriez-vous nous indiquer où vous avez acheté la bière et nous communiquer le numéro de lot (si possible) ?\\n\\n**Option 3 (Concis et direct) :**\\n\\n> Merci pour votre feedback. Nous allons investiguer votre signalement concernant une possible bière périmée ou contrefaite. Veuillez nous fournir le numéro de lot et le point de vente.\\n\\n\\nThe best option will depend on the context and the relationship with the reviewer.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Router Chain\n",
        "\n",
        "A Router Chain in LangChain is a specialized chain that dynamically routes user input to one of several predefined sub-chains based on the content or intent of the input. This allows developers to build intelligent, multi-skill systems that can select the appropriate response logic depending on the user’s needs."
      ],
      "metadata": {
        "id": "6BcI-Gbjt1mj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Up to this point, we have explored the use of LLMChain and SequentialChain. However, more complex scenarios often require additional logic. A common and practical pattern is routing input dynamically to different chains based on the nature or intent of that input.\n",
        "\n",
        "This can be effectively achieved using a Router Chain, which acts as a decision-making layer. It evaluates the input and determines which specialized subchain should handle the request. Each subchain is designed to process a specific category or type of input.\n",
        "\n",
        "For instance, consider a system that needs to route inputs based on the subject matter. You could define several prompt templates—each tailored to a specific domain. For example:\n",
        "\n",
        "- One prompt optimized for answering physics questions,\n",
        "\n",
        "- Another designed for math queries,\n",
        "\n",
        "- A third for history topics,\n",
        "\n",
        "- And a fourth for computer science.\n",
        "\n",
        "The router analyzes the input, determines the relevant subject area, and directs the input to the appropriate subchain using the corresponding prompt. Let's proceed by defining the prompt templates for each subject area."
      ],
      "metadata": {
        "id": "Dn-E-35-uJ_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "physics_template = \"\"\"You are a very smart physics professor. \\\n",
        "You are great at answering questions about physics in a concise\\\n",
        "and easy to understand manner. \\\n",
        "When you don't know the answer to a question you admit\\\n",
        "that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "\n",
        "math_template = \"\"\"You are a very good mathematician. \\\n",
        "You are great at answering math questions. \\\n",
        "You are so good because you are able to break down \\\n",
        "hard problems into their component parts,\n",
        "answer the component parts, and then put them together\\\n",
        "to answer the broader question.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "history_template = \"\"\"You are a very good historian. \\\n",
        "You have an excellent knowledge of and understanding of people,\\\n",
        "events and contexts from a range of historical periods. \\\n",
        "You have the ability to think, reflect, debate, discuss and \\\n",
        "evaluate the past. You have a respect for historical evidence\\\n",
        "and the ability to make use of it to support your explanations \\\n",
        "and judgements.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "\n",
        "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
        "You have a passion for creativity, collaboration,\\\n",
        "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
        "understanding of theories and algorithms, and excellent communication \\\n",
        "skills. You are great at answering coding questions. \\\n",
        "You are so good because you know how to solve a problem by \\\n",
        "describing the solution in imperative steps \\\n",
        "that a machine can easily interpret and you know how to \\\n",
        "choose a solution that has a good balance between \\\n",
        "time complexity and space complexity.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\""
      ],
      "metadata": {
        "id": "VJJYNbKOtV_0"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Once the prompt templates are defined, we can enhance them by assigning each one a name and a description. These metadata elements provide context about the purpose and specialization of each subchain. This information is then passed to the Router Chain, enabling it to make informed decisions about which subchain to route a given input to, based on the input’s content and intent."
      ],
      "metadata": {
        "id": "hegT7oUvuatq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_infos = [\n",
        "    {\n",
        "        \"name\": \"physics\",\n",
        "        \"description\": \"Good for answering questions about physics\",\n",
        "        \"prompt_template\": physics_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"math\",\n",
        "        \"description\": \"Good for answering math questions\",\n",
        "        \"prompt_template\": math_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"History\",\n",
        "        \"description\": \"Good for answering history questions\",\n",
        "        \"prompt_template\": history_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"computer science\",\n",
        "        \"description\": \"Good for answering computer science questions\",\n",
        "        \"prompt_template\": computerscience_template\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "mpkl-kQguUiz"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this scenario, we require a MultiPromptChain, which is specifically designed for routing between multiple prompt templates. As demonstrated, each routing option corresponds to a distinct prompt template. However, it’s important to note that routing is not limited to prompt templates alone—you can route between any type of chain, including those with additional logic or structure.\n",
        "\n",
        "To implement this, we will also introduce two additional components:\n",
        "\n",
        "- `LLMRouterChain`: This chain uses a language model to analyze the input and determine which subchain is most appropriate. The names and descriptions defined earlier for each subchain are used here to guide the routing decision.\n",
        "\n",
        "- `RouterOutputParser`: This utility parses the LLM's output into a structured dictionary format. It identifies the selected destination chain and extracts any relevant input variables, making the data suitable for downstream processing.\n",
        "\n",
        "With the necessary components in place, we can proceed to define and configure the language model that will be used for routing decisions. Let’s begin by importing and initializing the model."
      ],
      "metadata": {
        "id": "mA2_hGzCurUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "destination_chains = {}\n",
        "for p_info in prompt_infos:\n",
        "    name = p_info[\"name\"]\n",
        "    prompt_template = p_info[\"prompt_template\"]\n",
        "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    destination_chains[name] = chain\n",
        "\n",
        "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
        "destinations_str = \"\\n\".join(destinations)"
      ],
      "metadata": {
        "id": "GeRTntsiue1K"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As illustrated, each destination chain is implemented as an LLMChain, utilizing a language model with a specific prompt tailored to a particular subject area. In addition to these destination chains, a default chain must also be defined. This default chain serves as a fallback and is invoked when the Router Chain is unable to confidently determine which subchain is appropriate for the given input.\n",
        "\n",
        "For example, in the context of our earlier use case—where subchains handle physics, math, history, and computer science—the default chain would be triggered if the input question does not align with any of those predefined categories. This ensures graceful handling of unexpected or ambiguous inputs."
      ],
      "metadata": {
        "id": "0boGGGcJvKl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
        "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
      ],
      "metadata": {
        "id": "1XvFX8z-vD0O"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Next, we define the routing prompt template that will be used by the language model to determine which subchain to invoke. This template includes clear instructions on how the model should interpret the input and select the appropriate task, along with the expected output format. The output must follow a specific structure so that it can be accurately parsed by the RouterOutputParser and routed to the correct destination chain with the appropriate input."
      ],
      "metadata": {
        "id": "WMmslx4PvUV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
        "language model select the model prompt best suited for the input. \\\n",
        "You will be given the names of the available prompts and a \\\n",
        "description of what the prompt is best suited for. \\\n",
        "You may also revise the original input if you think that revising\\\n",
        "it will ultimately lead to a better response from the language model.\n",
        "\n",
        "<< FORMATTING >>\n",
        "Return a markdown code snippet with a JSON object formatted to look like:\n",
        "```json\n",
        "{{{{\n",
        "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
        "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
        "}}}}\n",
        "```\n",
        "\n",
        "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
        "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
        "well suited for any of the candidate prompts.\n",
        "REMEMBER: \"next_inputs\" can just be the original input \\\n",
        "if you don't think any modifications are needed.\n",
        "\n",
        "<< CANDIDATE PROMPTS >>\n",
        "{destinations}\n",
        "\n",
        "<< INPUT >>\n",
        "{{input}}\n",
        "\n",
        "<< OUTPUT (remember to include the ```json)>>\"\"\""
      ],
      "metadata": {
        "id": "6w342IjJvJzx"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Next, we construct a PromptTemplate using the routing instructions defined earlier. This prompt is then used to create the Router Chain by passing it, along with the configured language model, into the LLMRouterChain constructor. This setup enables the model to analyze inputs and determine the correct subchain based on the routing logic."
      ],
      "metadata": {
        "id": "Cg2l2ht0vlAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
        "router_prompt = PromptTemplate(\n",
        "    template=router_template,\n",
        "    input_variables=[\"input\"],\n",
        "    output_parser=RouterOutputParser(),\n",
        ")\n",
        "\n",
        "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
      ],
      "metadata": {
        "id": "ZM33EEqtvXrr"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is important to highlight the inclusion of the Router Output Parser in this setup. This component plays a crucial role by parsing the language model’s response, allowing the router chain to determine which subchain should handle the input.\n",
        "\n",
        "With all components defined, we can now assemble the overall multi-route chain. This final structure includes:\n",
        "\n",
        "- `The Router Chain`, responsible for evaluating input and determining the routing path.\n",
        "\n",
        "- `The Destination Chains`, each tailored to a specific task or subject.\n",
        "\n",
        "- `The Default Chain`, which handles any input that does not match the criteria for the defined destination chains.\n",
        "\n",
        "This configuration enables dynamic, intent-driven routing across multiple specialized processing chains."
      ],
      "metadata": {
        "id": "jw1R1u_0wJdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = MultiPromptChain(router_chain=router_chain,\n",
        "                         destination_chains=destination_chains,\n",
        "                         default_chain=default_chain, verbose=True\n",
        "                        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUd5BeV_vv_n",
        "outputId": "b9598c1c-5415-45b4-d7c7-fac9c72a95c1"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-54-3038952769.py:1: LangChainDeprecationWarning: Please see migration guide here for recommended implementation: https://python.langchain.com/docs/versions/migrating_chains/multi_prompt_chain/\n",
            "  chain = MultiPromptChain(router_chain=router_chain,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"What is Machine Learning?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "p4D_j4CCwTsX",
        "outputId": "90f5fb96-bf41-4dae-d8b4-20a31d0a87c8"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "computer science: {'input': 'What is Machine Learning?'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Machine learning (ML) is a branch of artificial intelligence (AI) and computer science which focuses on the use of data and algorithms to imitate the way that humans learn, gradually improving its accuracy.  Instead of being explicitly programmed to perform a task, a machine learning system learns from data.  This learning process involves identifying patterns, making predictions, and improving its performance over time without explicit human intervention for each new input.\\n\\nHere's a breakdown in imperative steps, illustrating the core process:\\n\\n1. **Data Acquisition and Preparation:** Gather relevant data. This might involve collecting data from various sources, cleaning it (handling missing values, outliers, etc.), and transforming it into a suitable format for the chosen algorithm (e.g., normalization, feature engineering).\\n\\n2. **Algorithm Selection:** Choose a machine learning algorithm appropriate for the task.  This depends on several factors including the type of data (structured, unstructured), the desired outcome (classification, regression, clustering), and the available computational resources.  Examples include: linear regression, logistic regression, support vector machines (SVMs), decision trees, random forests, neural networks, etc.\\n\\n3. **Model Training:**  Feed the prepared data into the chosen algorithm. The algorithm uses this data to learn the underlying patterns and relationships. This process involves adjusting the algorithm's internal parameters to minimize the difference between its predictions and the actual values in the data (this is often referred to as minimizing a loss function).\\n\\n4. **Model Evaluation:** Assess the performance of the trained model using appropriate metrics.  These metrics depend on the task (e.g., accuracy, precision, recall, F1-score for classification; mean squared error, R-squared for regression).  This often involves splitting the data into training, validation, and testing sets.  The validation set is used to tune hyperparameters and prevent overfitting (where the model performs well on the training data but poorly on unseen data).  The testing set provides a final, unbiased evaluation of the model's performance.\\n\\n5. **Model Deployment and Monitoring:** Deploy the trained model to make predictions on new, unseen data.  Continuously monitor the model's performance and retrain it periodically with new data to maintain accuracy and adapt to changing patterns.\\n\\n\\nThe choice of algorithm and the specific implementation details will significantly impact the time and space complexity of the solution.  For example, a simple linear regression model will generally have lower complexity than a deep neural network.  A skilled machine learning engineer carefully considers this trade-off when designing and implementing a solution.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"what is 2 + 2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "hsPxygDxwWKw",
        "outputId": "ced838b9-3aac-4ee3-9129-ae63e2ff9b7b"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "math: {'input': 'what is 2 + 2'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The question is: What is 2 + 2?\\n\\nThis is a simple addition problem.\\n\\n**Component Part 1:**  The number 2 represents two units.\\n\\n**Component Part 2:** The \"+\" symbol indicates addition, meaning we combine the units.\\n\\n**Component Part 3:** Combining two units and two more units results in a total of four units.\\n\\n**Answer:** Therefore, 2 + 2 = 4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"What is black body radiation?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "jeZFLPg_wmkB",
        "outputId": "9680ff11-4955-43e8-dfd7-c8c504c3124d"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "physics: {'input': 'What is black body radiation?'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Black body radiation is the electromagnetic radiation emitted by an idealized object, called a black body, that perfectly absorbs all incoming radiation at all wavelengths.  It's characterized by a specific spectrum that depends only on the object's temperature.  The hotter the object, the more intensely it radiates, and the shorter the wavelengths at which it emits most strongly.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"Why does every cell in our body contain DNA?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "eMyMgQcewoQ8",
        "outputId": "5c871b27-8cae-44aa-f664-f226019b789e"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "None: {'input': 'Why does every cell in our body contain DNA?'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Every cell in our body contains DNA because DNA holds the instructions for building and maintaining the entire organism.  It's the blueprint for life.  These instructions are crucial for:\\n\\n* **Protein synthesis:** DNA contains the genes that code for the production of proteins. Proteins are the workhorses of the cell, carrying out a vast array of functions, from structural support to enzymatic activity.  Every cell needs to produce proteins to function.\\n\\n* **Cell division and replication:** When a cell divides, it needs to pass on a complete copy of its DNA to each daughter cell. This ensures that each new cell has the necessary instructions to function correctly.\\n\\n* **Cellular regulation and maintenance:** DNA plays a role in regulating gene expression, determining which proteins are produced and when. This is vital for maintaining the cell's health and function, and for responding to changes in the environment.\\n\\n* **Inheritance:** The DNA we inherit from our parents determines many of our characteristics, from our eye color to our susceptibility to certain diseases. This inherited information is passed on through the DNA in our cells.\\n\\nIn short, without DNA, cells wouldn't know what to do, how to grow, how to repair themselves, or how to reproduce.  It's the fundamental operating system for every living cell in our body.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"When was The World War I?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "ILNh52yrwqQm",
        "outputId": "f3b45876-0bdf-4e78-85e0-f5e4ae4375b7"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "History: {'input': 'When was World War I?'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"World War I took place from **1914 to 1918**.  More specifically, the war began on 28 July 1914 with Austria-Hungary's declaration of war on Serbia, following the assassination of Archduke Franz Ferdinand, and concluded with the Armistice of 11 November 1918.  While the fighting largely ceased on that date, the formal peace treaties that ended the state of war were not signed until the following year.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    }
  ]
}